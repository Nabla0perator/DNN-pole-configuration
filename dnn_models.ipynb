{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP1L(chainer.Chain):\n",
    "    def __init__(self, nodes_l1):\n",
    "        self.desc = '[{:003d}]'.format(nodes_l1) #one hidden layer\n",
    "        super(MLP1L, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, nodes_l1, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l2 = L.Linear(None, 35, nobias=False, initialW=None, initial_bias=None)\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        return self.l2(h1)\n",
    "\n",
    "class MLP2L(chainer.Chain):\n",
    "    def __init__(self, nodes_l1, nodes_l2):\n",
    "        self.desc = '[{:003d}-{:003d}]'.format(nodes_l1, nodes_l2) #two hidden layers\n",
    "        super(MLP2L, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, nodes_l1, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l2 = L.Linear(None, nodes_l2, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l3 = L.Linear(None, 35, nobias=False, initialW=None, initial_bias=None)\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)\n",
    "\n",
    "class MLP3L(chainer.Chain):\n",
    "    def __init__(self, nodes_l1, nodes_l2, nodes_l3):\n",
    "        self.desc = '[{:003d}-{:003d}-{:003d}]'.format(nodes_l1, nodes_l2, nodes_l3) #three hidden layers\n",
    "        super(MLP3L, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, nodes_l1, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l2 = L.Linear(None, nodes_l2, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l3 = L.Linear(None, nodes_l3, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l4 = L.Linear(None, 35, nobias=False, initialW=None, initial_bias=None)\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        h3 = F.relu(self.l3(h2))\n",
    "        return self.l4(h3)\n",
    "\n",
    "class MLP4L(chainer.Chain):\n",
    "    def __init__(self, nodes_l1, nodes_l2, nodes_l3, nodes_l4):\n",
    "        self.desc = '[{:003d}-{:003d}-{:003d}-{:003d}]'.format(nodes_l1, nodes_l2, nodes_l3, nodes_l4) #four hidden layers\n",
    "        super(MLP4L, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, nodes_l1, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l2 = L.Linear(None, nodes_l2, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l3 = L.Linear(None, nodes_l3, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l4 = L.Linear(None, nodes_l4, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l5 = L.Linear(None, 35, nobias=False, initialW=None, initial_bias=None)\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        h3 = F.relu(self.l3(h2))\n",
    "        h4 = F.relu(self.l4(h3))\n",
    "        return self.l5(h4)\n",
    "    \n",
    "class MLP5L(chainer.Chain):\n",
    "    def __init__(self, nodes_l1, nodes_l2, nodes_l3, nodes_l4, nodes_l5):\n",
    "        self.desc = '[{:003d}-{:003d}-{:003d}-{:003d}-{:003d}]'.format(nodes_l1, nodes_l2, nodes_l3, nodes_l4, nodes_l5) #five hidden layers\n",
    "        super(MLP5L, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, nodes_l1, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l2 = L.Linear(None, nodes_l2, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l3 = L.Linear(None, nodes_l3, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l4 = L.Linear(None, nodes_l4, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l5 = L.Linear(None, nodes_l5, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l6 = L.Linear(None, 35, nobias=False, initialW=None, initial_bias=None)\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        h3 = F.relu(self.l3(h2))\n",
    "        h4 = F.relu(self.l4(h3))\n",
    "        h5 = F.relu(self.l5(h4))\n",
    "        return self.l6(h5)\n",
    "    \n",
    "class MLP6L(chainer.Chain):\n",
    "    def __init__(self, nodes_l1, nodes_l2, nodes_l3, nodes_l4, nodes_l5, nodes_l6):\n",
    "        self.desc = '[{:003d}-{:003d}-{:003d}-{:003d}-{:003d}-{:003d}]'.format(nodes_l1, nodes_l2, nodes_l3, nodes_l4, nodes_l5, nodes_l6) #six hidden layers\n",
    "        super(MLP6L, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, nodes_l1, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l2 = L.Linear(None, nodes_l2, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l3 = L.Linear(None, nodes_l3, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l4 = L.Linear(None, nodes_l4, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l5 = L.Linear(None, nodes_l5, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l6 = L.Linear(None, nodes_l6, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l7 = L.Linear(None, 35, nobias=False, initialW=None, initial_bias=None)\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        h3 = F.relu(self.l3(h2))\n",
    "        h4 = F.relu(self.l4(h3))\n",
    "        h5 = F.relu(self.l5(h4))\n",
    "        h6 = F.relu(self.l6(h5))\n",
    "        return self.l7(h6)\n",
    "\n",
    "class MLP4LDRP(chainer.Chain):\n",
    "    def __init__(self, nodes, drprate2, drprate3, drprate4):\n",
    "        self.desc = '[{:03d}:{:.02f}-{:.02f}-{:.02f}]'.format(nodes, drprate2, drprate3, drprate4) #four hidden layers\n",
    "        self.drprate2 = drprate2\n",
    "        self.drprate3 = drprate3\n",
    "        self.drprate4 = drprate4\n",
    "        super(MLP4LDRP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, nodes, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l2 = L.Linear(None, nodes, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l3 = L.Linear(None, nodes, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l4 = L.Linear(None, nodes, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l5 = L.Linear(None, 35, nobias=False, initialW=None, initial_bias=None)\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        #no dropout here (we need all the experimental data points)\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        d2 = F.dropout(h2, ratio=self.drprate2)\n",
    "        h3 = F.relu(self.l3(d2))\n",
    "        d3 = F.dropout(h3, ratio=self.drprate3)\n",
    "        h4 = F.relu(self.l4(d3))\n",
    "        d4 = F.dropout(h4, ratio=self.drprate4)\n",
    "        return self.l5(d4)    \n",
    "    \n",
    "class MLP5LDRP(chainer.Chain):\n",
    "    #Use this if you wish the model to adjust to new classes\n",
    "    def __init__(self, nodes, drprate2, drprate3, drprate4, drprate5):\n",
    "        self.desc = '[{:03d}:{:.02f}-{:.02f}-{:.02f}-{:.02f}]'.format(nodes, drprate2, drprate3, drprate4, drprate5) #five hidden layers\n",
    "        self.drprate2 = drprate2\n",
    "        self.drprate3 = drprate3\n",
    "        self.drprate4 = drprate4\n",
    "        self.drprate5 = drprate5\n",
    "        super(MLP5LDRP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, nodes, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l2 = L.Linear(None, nodes, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l3 = L.Linear(None, nodes, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l4 = L.Linear(None, nodes, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l5 = L.Linear(None, nodes, nobias=False, initialW=None, initial_bias=None)\n",
    "            self.l6 = L.Linear(None, 35, nobias=False, initialW=None, initial_bias=None)\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        #no dropout here (we need all the experimental data points)\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        d2 = F.dropout(h2, ratio=self.drprate2)\n",
    "        h3 = F.relu(self.l3(d2))\n",
    "        d3 = F.dropout(h3, ratio=self.drprate3)\n",
    "        h4 = F.relu(self.l4(d3))\n",
    "        d4 = F.dropout(h4, ratio=self.drprate4)\n",
    "        h5 = F.relu(self.l5(d4))\n",
    "        d5 = F.dropout(h5, ratio=self.drprate5)\n",
    "        return self.l6(d5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
